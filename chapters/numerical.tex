% =============================================================================
\chapter{Numerical methods}
\label{cha:appendix:numerical}
% =============================================================================

This Appendix outlines the approach to numerical simulations performed for this thesis.
While full texts of the programs used are too long to include verbatim in the text, the main choices made will be described here.


% =============================================================================
\section{Parallel calculations}
% =============================================================================

The phase space methods described in this thesis are inherently parallel.
Such algorithms are suitable to run on modern graphical processing units (\abbrev{gpu}s), as long as the data being processed fits the video memory (which was our case).
We chose nVidia's CUDA as the general purpose \abbrev{gpu} (\abbrev{gpgpu}) platform because of its maturity and the included set of libraries with effective implementations of the \abbrev{fft}, random number generators, and other useful algorithms.
In practice, OpenCL could be used as well: at the moment it is just as fast as CUDA on nVidia video cards, and has an additional benefit of supporting AMD cards.

Furthermore, in order to speed up prototyping we did not use the CUDA language itself (which is, essentially, a superset of C++), but Python bindings to it.
Python is a general-purpose dynamically typed language with a rich set of third-party libraries.
It is quite popular in the academic community owing to the excellent NumPy and SciPy packages~\cite{Oliphant2007}, which provide an extensive toolset for numerical calculations.
PyCUDA~\cite{Klockner2012} augments it with a convenient access to CUDA and its libraries, significantly reducing the amount of boilerplate code and making metaprogramming possible.
Of course, the dynamical facilities of Python make it slower than C++, but this was negligible in our simulations since the majority of calculations was done on \abbrev{gpu}, and the Python language overhead was masked by their asynchronous execution.

The use of \abbrev{gpu} hardware allowed us to reach a hundredfold speedup of calculations on a single nVidia Tesla C2050 as compared to MatLab implementations.
This was improved even further by combining the results calculated on several video cards, such as we did for the calculations in \secref{bell-ineq:ghz}, where we used seven nVidia Tesla M2090.


% =============================================================================
\section{XMDS}
% =============================================================================

For the cases where the raw speed of calculations was less important, and a custom \abbrev{gpgpu} program was not required, we used the XMDS package~\cite{Collecutt2001,Dennis2013}.
XMDS is a code generator for numerical simulation problems, written in Python.
It creates and compiles a C++ program that performs the integration, based on a brief \abbrev{xml} description of the simulation problem.
The strength of XMDS lies in highly optimized integration algorithms it uses (which is improved even more by on-demand compilation), and in its transparent usage of multi-processor interface (\abbrev{mpi}), which allows it to take advantage of multiple cores of a \abbrev{cpu}, or multiple nodes in a cluster.
We used XMDS to integrate the \abbrev{sde}s in \charef{exact}.


% =============================================================================
\section{Stochastic integration}
% =============================================================================

The problem of integrating \abbrev{sde}s is well-studied; for some general information, one can refer to an extensive reviews by Drummond and Mortimer~\cite{Drummond1991}, and by Werner and Drummond~\cite{Werner1997}.
The common approach is to use a method from the Runge-Kutta family, which can be applied without changes to a set of \abbrev{sde}s in the Stratonovich form~\cite{Wilkie2004,Wilkie2005}.

It is possible to employ variable-stepsize methods, which are strongly convergent, provided that stochastic differentials on the subintervals are sampled correctly~\cite{Wilkie2005}.
Another widely-used specialized Runge-Kutta subtype is the \abbrev{rk4} interaction picture method~\cite{CaradocDavies2000}, which can improve convergence properties considerably when applied to stiff equations.
One must note though that the reduction in the number of steps required for a given level of convergence is often neutralized by the increased number of \abbrev{fft}s that have to be performed on each step.

In our case, neither adaptive stepsize, nor a transition to the interaction picture was not necessary, and we settled on a fixed-stepsize fourth order \abbrev{rk} method optimized for low dissipation, low dispersion and low storage~\cite{Berland2006}.
In particular, the latter means that in each substep of the \abbrev{rk} propagation the result depends only on the output of the previous step.
This results in an algorithm requiring 6 steps in order to reach 4th order.
The low-storage property is very important in the \abbrev{gpgpu} world, where one has to keep whole arrays of intermediate results in video memory.


% =============================================================================
\section{Software developed for this thesis}
% =============================================================================

In the process of writing this thesis we have created several libraries used by our simulation programs.
While usefulness of these libraries may decrease with time quickly (in case we choose not to maintain them), and many of the sources are not very well documented, we still feel that it is necessary to reference them here.

The programs used to obtain and process the numerical results in this thesis can be found in the Git repository \href{http://github.com/Manticore/thesis}{github.com/Manticore/thesis}, along with the sources of the thesis itself.
This includes both Python programs and several XMDS scripts used in \charef{exact}.

The processing code for the XMDS scripts was generated by a symbolic calculation library \href{http://github.com/Manticore/wigner}{github.com/Manticore/wigner}, written in Haskell.
This library is able to perform the Wigner transformation of arbitrary operator expressions (including the ones with field operators) and conversions between normally and symmetrically ordered operator products.
In addition to \charef{exact}, it was used to test the theorems from \charef{wigner-spec} and also in several other places in the thesis where ordering conversions of complex operator expressions were needed.

To obtain the results in \charef{bec-noise} and \charef{bec-squeezing} we created \href{http://github.com/Manticore/beclab}{github.com/Manticore/beclab}Â a framework in Python and CUDA for simulating the dynamics of trapped \abbrev{bec}s.
The low-level part of this library later gave rise to \href{http://github.com/Manticore/reikna}{github.com/Manticore/reikna}, a code generator for \abbrev{gpgpu} algorithms written in Python, which can work both with CUDA and OpenCL.
